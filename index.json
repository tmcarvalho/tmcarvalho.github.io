[{"authors":null,"categories":null,"content":"I defended my Ph.D. at FCUP in Computer Science with a research focus on data privacy and utility preservation of machine learning models. My work centers on developing and evaluating methods that enable the responsible use of AI, with a particular interest in privacy-preserving techniques and regulatory compliance. Passionate about trustworthy AI, I\u0026rsquo;m committed to advancing technologies that are not only private but also fair and explainable.\n Download my resumé. -- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I defended my Ph.D. at FCUP in Computer Science with a research focus on data privacy and utility preservation of machine learning models. My work centers on developing and evaluating methods that enable the responsible use of AI, with a particular interest in privacy-preserving techniques and regulatory compliance.","tags":null,"title":"","type":"authors"},{"authors":["Pedro Santos","Tânia Carvalho","Filipe Magalhães","Luís Antunes"],"categories":null,"content":"","date":1738368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738368000,"objectID":"064e098a776a876458237b0e6ed7c0b4","permalink":"https://tmcarvalho.github.io/publication/securevisualfl/","publishdate":"2025-02-01T00:00:00Z","relpermalink":"/publication/securevisualfl/","section":"publication","summary":"As the demand for privacy in visual data management grows, safeguarding sensitive information has become a critical challenge. This paper addresses the need for privacy-preserving solutions in large-scale visual data processing by leveraging federated learning. Although there have been developments in this field, previous research has mainly focused on integrating object detection with either anonymization or federated learning. However, these pairs often fail to address complex privacy concerns. On the one hand, object detection with anonymization alone can be vulnerable to reverse techniques. On the other hand, federated learning may not provide sufficient privacy guarantees. Therefore, we propose a new approach that combines object detection, federated learning and anonymization. Combining these three components aims to offer a robust privacy protection strategy by addressing different vulnerabilities in visual data. Our solution is evaluated against traditional centralized models, showing that while there is a slight trade-off in accuracy, the privacy benefits are substantial, making it well-suited for privacy sensitive applications.","tags":[],"title":"Secure Visual Data Processing via Federated Learning","type":"publication"},{"authors":["Tânia Carvalho","Luís Antunes","Cristina Costa","Nuno Moniz"],"categories":null,"content":"","date":1738195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738195200,"objectID":"00deb0d423b388538741151c6e6c2ffb","permalink":"https://tmcarvalho.github.io/publication/opendata/","publishdate":"2025-01-30T00:00:00Z","relpermalink":"/publication/opendata/","section":"publication","summary":"The Covid-19 pandemic has affected the world at multiple levels. Data sharing was pivotal for advancing research to understand the underlying causes and implement effective containment strategies. In response, many countries have promoted the availability of daily cases to support research initiatives, fostering collaboration between organisations and making such data available to the public through open data platforms. Despite the several advantages of data sharing, one of the major concerns before releasing health data is its impact on individuals' privacy. Such a sharing process should be based on state-of-the-art methods in Data Protection by Design and by Default. In this paper, we use a data set related to Covid-19 cases in the second largest hospital in Portugal to show how it is feasible to ensure data privacy while improving the quality and maintaining the utility of the data. Our goal is to demonstrate how knowledge exchange in multidisciplinary teams of healthcare practitioners, data privacy, and data science experts is crucial to co-developing strategies that ensure high utility of de-identified data.","tags":[],"title":"Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach","type":"publication"},{"authors":["Filipa Lopes","Carolina Trindade","Tânia Carvalho","Maria Almeida","Ana Sofia Carvalho"],"categories":null,"content":"","date":1734480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734480000,"objectID":"b04186a578113d2bde3f196b5fd26c8d","permalink":"https://tmcarvalho.github.io/publication/repoeu_protocol/","publishdate":"2024-12-18T00:00:00Z","relpermalink":"/publication/repoeu_protocol/","section":"publication","summary":"Within the scope of clinical trials, developing participant information sheets and informed consent forms is a complex task that demands clarity, precision, and compliance with regulatory standards. Developing these documents is crucial for ensuring that participants are fully informed about the research in which they are involved. However, the process is often time-consuming and resource-intensive. In this context, we present the development of a methodology enabling the use of Large Language Models to assist in the creation of information sheets and informed consent forms for clinical trials according to a predesigned template. This research is being conducted within the framework of the project REPO4EU (Precision drug REPurpOsing For EUrope and the world).","tags":[],"title":"Developing Project-Specific Consent Documents: A Registered Report for a Multistep Approach Using LLMs","type":"publication"},{"authors":null,"categories":null,"content":"Problem: Public transportation is a crucial component to every city, and a poor management of this resource can greatly affect citizens day-to-day lives by creating traffic, pollution, and population satisfaction in general.\n ","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"1763b36d5d4e5ef58d807cfa0e2b5f69","permalink":"https://tmcarvalho.github.io/collaborations/hackacity/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/collaborations/hackacity/","section":"collaborations","summary":"I was a mentor at the 7th Hack a City event, where I provided guidance and support to participants to improve the city of Porto's data safely. I was also responsible for the prior de-identification of such data.","tags":null,"title":"Business Mentor","type":"collaborations"},{"authors":["Carolina Trindade","Luís Antunes","Tânia Carvalho","Nuno Moniz"],"categories":null,"content":"","date":1726185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726185600,"objectID":"98e4cd495698ae7bf5083dd130217151","permalink":"https://tmcarvalho.github.io/publication/psd/","publishdate":"2024-09-13T00:00:00Z","relpermalink":"/publication/psd/","section":"publication","summary":"Multiple synthetic data generation models have emerged, among which deep learning models have become the vanguard due to their ability to capture the underlying characteristics of the original data. However, the resemblance of the synthetic to the original data raises important questions on the protection of individuals' privacy. As synthetic data is perceived as a means to fully protect personal information, most current related work disregards the impact of re-identification risk. In particular, limited attention has been given to exploring outliers, despite their privacy relevance. In this work, we analyze the privacy of synthetic data w.r.t the outliers. Our main findings suggest that outliers re-identification via linkage attack is feasible and easily achieved. Furthermore, additional safeguards such as differential privacy can prevent re-identification, albeit at the expense of the data utility.","tags":[],"title":"Synthetic Data Outliers: Navigating Identity Disclosure","type":"publication"},{"authors":["Tânia Carvalho","Nuno Moniz","Luís Antunes"],"categories":null,"content":"","date":1719273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"1d9a9f0266f31aa587d23444dbae301f","permalink":"https://tmcarvalho.github.io/publication/autopriv/","publishdate":"2024-06-25T00:00:00Z","relpermalink":"/publication/autopriv/","section":"publication","summary":"Sharing private data for learning tasks is pivotal for transparent and secure machine learning applications. Many privacy-preserving techniques have been proposed for this task aiming to transform the data while ensuring the privacy of individuals. Some of these techniques have been incorporated into tools, whereas others are accessed through various online platforms. However, such tools require manual configuration, which can be complex and time-consuming. Moreover, they require substantial expertise, potentially restricting their use to those with advanced technical knowledge. In this paper, we propose AUTOPRIV, the first automated privacy-preservation method, that eliminates the need for any manual configuration. AUTOPRIV employs meta-learning to automate the de-identification process, facilitating the secure release of data for machine learning tasks. The main goal is to anticipate the predictive performance and privacy risk of a large set of privacy configurations. We provide a ranked list of the most promising solutions, which are likely to achieve an optimal approximation within a new domain. AUTOPRIV is highly effective as it reduces computational complexity and energy consumption considerably.","tags":[],"title":"Automated Privacy-Preserving Techniques via Meta-Learning","type":"publication"},{"authors":null,"categories":null,"content":"This project is a collaboration between School of Medicine and Biomedical Sciences, Faculty of Sciences and TekPrivacy.\n ","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"5aeefe40f605a33613507941990e1bdb","permalink":"https://tmcarvalho.github.io/collaborations/repoeu/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/collaborations/repoeu/","section":"collaborations","summary":"I investigated the use of LLMs to assist in the creation of information sheets and informed consent forms for clinical trials. This study is conducted within the context of the research project REPO4EU (Precision drug REPurpOsing For EUrope and the world).","tags":null,"title":"Collaborator","type":"collaborations"},{"authors":["Filipa Lopes","Carolina Trindade","Tânia Carvalho","Maria Almeida","Ana Sofia Carvalho"],"categories":null,"content":"","date":1714694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714694400,"objectID":"ead78a99c1a0ad7b015eac665f2567e9","permalink":"https://tmcarvalho.github.io/publication/repoeu/","publishdate":"2024-05-03T00:00:00Z","relpermalink":"/publication/repoeu/","section":"publication","summary":"We present the development of a methodology enabling the use of Large Language Models (LLMs) to assist in the creation of information sheets and informed consent forms for clinical trials. This study is conducted within the context of the research project REPO4EU (Precision drug REPurpOsing For EUrope and the world). REPO4EU is committed to adhering to both national and international legislation and ethical and privacy best practices. The project follows an ethics and privacy-by-design approach, addressing all ethical, legal, social, and safety concerns arising from its research. As part of its commitment to research ethics, this study aims to create project-specific templates for information sheets and informed consent forms and explore the potential of LLMs to facilitate this process.\nFor this purpose, we used a multi-step methodological approach and two pilot exercises have been conducted for REPO-HYPER II clinical trial. A bibliographic review to identify the essential elements of information sheets and informed consent forms for clinical trials has been performed (Tam et al., 2015; O'Sullivan et al., 2020; Pietrzykowski, et al., 2021; Solomon et al., 2021; Wu et al., 2024). This included peer-reviewed articles and relevant guidelines. Additionally, grey literature sources, such as reports and white papers from regulatory bodies, were explored. The next step was the development of a preliminary checklist to capture the key information required for both documents based on the bibliographic review. This checklist was then validated through analysis of 15 information sheets and informed consent forms of ongoing clinical trials registered in the EU Clinical Trials database. The focus of this analysis was on how these trials addressed the information needs of potential participants. The initial checklist was subsequently adjusted following the analysis of existing clinical trial documents.\nFurthermore, an additional task involved the development of a general template for the information sheet and informed consent forms for this clinical trial. These templates have been based on a template provided by the Swedish regulatory authority and supplemented with all the requirements identified in the previous exercise. Moreover, the recommendations outlined by Coleman et al. (2021) were taken into account in order to improve readability and, consequently, enhance the decision-making capacity of the participants.\nHowever, the analysis and the design of these documents can be time-consuming, as each item and document must be scrutinised in detail. Therefore, we aim to leverage LLMs (Large Language Models) to assist in the creation and/or analysis of both the information sheet and the informed consent documents. LLMs are a type of deep learning model that are trained on large amounts of data to learn and generate text that mimics the natural language (Chang et al., 2024). As such, LLMs can be used in the future REPO4EU platform to automatically validate all items in information sheets and informed consent forms and to help the creation of these documents (Clusmann et al., 2023). This could lead to significant time savings, allowing clinical practitioners and researchers to focus more on their relationship with research participants and to have more flexible documents for participants with different levels of literacy. The implemented methodology could be transferable to other contexts, beyond REPO4EU and represents an ethics by design outcome of this project.","tags":[],"title":"Developing Project-Specific Informed Consent Forms: A Multi-Step Approach within the REPO4EU Framework","type":"publication"},{"authors":null,"categories":null,"content":"ε-PrivateSMOTE is a technique designed for safeguarding against re-identification and linkage attacks, particularly addressing cases with a high re-identification risk. It generates synthetic data via noise-induced interpolation with differential privacy principles to obfuscate high-risk cases. ε-PrivateSMOTE allows having new cases similar to the originals while preserving privacy and maximising predictive utility. Most importanly, ε-PrivateSMOTE is a resource efficient and less time-consuming than conventional de-identification approaches such as deep learning and differential privacy-based solutions.\n","date":1705708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705708800,"objectID":"081488b03bb2ff5f5c44322cf3271ba7","permalink":"https://tmcarvalho.github.io/project/privatesmote/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/project/privatesmote/","section":"project","summary":"Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control","tags":null,"title":"ε-PrivateSMOTE","type":"project"},{"authors":["Tânia Carvalho","Nuno Moniz","Luís Antunes"],"categories":null,"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"60f77dc76845b4b0fc1ab8801cb522dc","permalink":"https://tmcarvalho.github.io/publication/epia/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/epia/","section":"publication","summary":"As the frontier of machine learning applications moves further into human interaction, multiple concerns arise regarding automated decision-making. Two of the most critical issues are fairness and data privacy. On the one hand, one must guarantee that automated decisions are not biased against certain groups, especially those unprotected or marginalized. On the other hand, one must ensure that the use of personal information fully abides by privacy regulations and that user identities are kept safe. The balance between privacy, fairness, and predictive performance is complex. However, despite their potential societal impact, we still demonstrate a poor understanding of the dynamics between these optimization vectors. In this paper, we study this three-way tension and how the optimization of each vector impacts others, aiming to inform the future development of safe applications. In light of claims that predictive performance and fairness can be jointly optimized, we find this is only possible at the expense of data privacy. Overall, experimental results show that one of the vectors will be penalized regardless of which of the three we optimize. Nonetheless, we find promising avenues for future work in joint optimization solutions, where smaller trade-offs are observed between the three vectors.","tags":[],"title":"A Three-Way Knot: Privacy, Fairness, and Predictive Performance Dynamics","type":"publication"},{"authors":null,"categories":null,"content":"Statistical Disclosure Control seeks to reduce the risk of confidential information disclosure by de-identifying them. Such de-identification is guaranteed through privacy-preserving techniques (PPTs). However, de-identified data usually results in loss of information, with a possible impact on data analysis precision and model predictive performance. This course covers the de-identification process which aims to protect the individual’s privacy while maintaining the interpretability of the data (i.e., its usefulness). The program includes:\n Introduction to Python Introduction to data privacy Predictive modeling Privacy-preserving techniques Training Current directions on data privacy  ","date":1682467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682467200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://tmcarvalho.github.io/project/example/","publishdate":"2023-04-26T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Practical course covering the basic principles of the data de-identification process.","tags":null,"title":"Hands-on data de-identification","type":"project"},{"authors":["Tânia Carvalho","Nuno Moniz","Pedro Faria","Luís Antunes"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"55ac6d9e94bcbc343a610569feec387b","permalink":"https://tmcarvalho.github.io/publication/eswa/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/publication/eswa/","section":"publication","summary":"Machine learning is increasingly used in the most diverse applications and domains, whether in healthcare, to predict pathologies, or in the financial sector to detect fraud. One of the linchpins for efficiency and accuracy in machine learning is data utility. However, when it contains personal information, full access may be restricted due to laws and regulations aiming to protect individuals' privacy. Therefore, data owners must ensure that any data shared guarantees such privacy. Removal or transformation of private information (de-identification) are among the most common techniques. Intuitively, one can anticipate that reducing detail or distorting information would result in losses for model predictive performance. However, previous work concerning classification tasks using de-identified data generally demonstrates that predictive performance can be preserved in specific applications. In this paper, we aim to evaluate the existence of a trade-off between data privacy and predictive performance in classification tasks. We leverage a large set of privacy-preserving techniques and learning algorithms to provide an assessment of re-identification ability and the impact of transformed variants on predictive performance. Unlike previous literature, we confirm that the higher the level of privacy (lower re-identification risk), the higher the impact on predictive performance, pointing towards clear evidence of a trade-off.","tags":[],"title":"Towards a Data Privacy-Predictive Performance Trade-off","type":"publication"},{"authors":["Tânia Carvalho","Nuno Moniz","Pedro Faria","Luís Antunes"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"c613964539060f72b7b5ca0b841b7b61","permalink":"https://tmcarvalho.github.io/publication/survey/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/survey/","section":"publication","summary":"The exponential growth of collected, processed, and shared microdata has given rise to concerns about individuals’ privacy. As a result, laws and regulations have emerged to control what organisations do with microdata and how they protect it. Statistical Disclosure Control seeks to reduce the risk of confidential information disclosure by de-identifying them. Such de-identification is guaranteed through privacy-preserving techniques (PPT). However, de-identified data usually results in loss of information, with a possible impact on data analysis precision and model predictive performance. The main goal is to protect the individual’s privacy while maintaining the interpretability of the data, i.e. its usefulness. Statistical Disclosure Control is an area that is expanding and needs to be explored since there is still no solution that guarantees optimal privacy and utility. This survey focuses on all steps of the de-identification process. We present existing PPT used in microdata de-identification, privacy measures suitable for several disclosure types and, information loss and predictive performance measures. In this survey, we discuss the main challenges raised by privacy constraints, describe the main approaches to handle these obstacles, review the taxonomies of PPT, provide a theoretical analysis of existing comparative studies, and raise multiple open issues.","tags":[],"title":"Survey on Privacy-Preserving Techniques for Data Publishing","type":"publication"},{"authors":["Tânia Carvalho","Nuno Moniz","Pedro Faria","Luís Antunes","Nitesh Chawla"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"875e2af20d9dc053e23efade61db1c5c","permalink":"https://tmcarvalho.github.io/publication/privatesmote/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/privatesmote/","section":"publication","summary":"We can protect user data privacy via many approaches, such as statistical transformation or generative models. However, each of them has critical drawbacks. On the one hand, creating a transformed data set using conventional techniques is highly time-consuming. On the other hand, in addition to long training phases, recent deep learning-based solutions require significant computational resources. In this paper, we propose PrivateSMOTE, a technique designed for competitive effectiveness in protecting cases at maximum risk of re-identification while requiring much less time and computational resources. It works by synthetic data generation via interpolation to obfuscate high-risk cases while minimizing data utility loss of the original data. Compared to multiple conventional and state-of-the-art privacy-preservation methods on 20 data sets, PrivateSMOTE demonstrates competitive results in re-identification risk. Also, it presents similar or higher predictive performance than the baselines, including generative adversarial networks and variational autoencoders, reducing their energy consumption and time requirements by a minimum factor of 9 and 12, respectively.","tags":[],"title":"Privacy-Preserving Data Synthetisation for Secure Information Sharing","type":"publication"},{"authors":["Tânia Carvalho","Nuno Moniz","Pedro Faria","Luís Antunes"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"49e6103a81f4ff22b25408ee418d952b","permalink":"https://tmcarvalho.github.io/publication/plos/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/plos/","section":"publication","summary":"Faced with the emergence of the Covid-19 pandemic, and to better understand and contain the disease’s spread, health organisations increased the collaboration with other organisations sharing health data with data scientists and researchers. Data analysis assists such organisations in providing information that could help in decision-making processes. For this purpose, both national and regional health authorities provided health data for further processing and analysis. Shared data must comply with existing data protection and privacy regulations. Therefore, a robust de-identification procedure must be used, and a re-identification risk analysis should also be performed. De-identified data embodies state-of-the-art approaches in Data Protection by Design and Default because it requires the protection of direct and indirect identifiers (not just direct). This article highlights the importance of assessing re-identification risk before data disclosure by analysing a data set of individuals infected by Covid-19 that was made available for research purposes. We stress that it is highly important to make this data available for research purposes and that this process should be based on the state of the art methods in Data Protection by Design and by Default. Our main goal is to consider different re-identification risk analysis scenarios since the information on the intruder side is unknown. Our conclusions show that there is a risk of identity disclosure for all of the studied scenarios. For one, in particular, we proceed to an example of a re-identification attack. The outcome of such an attack reveals that it is possible to identify individuals with no much effort.","tags":[],"title":"Fundamental privacy rights in a pandemic state","type":"publication"},{"authors":["Tânia Carvalho","Nuno Moniz"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"db2045ab803a6b08c2facf151ea52efe","permalink":"https://tmcarvalho.github.io/publication/ida/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/ida/","section":"publication","summary":"Privacy-preservation has become an essential concern in many data mining applications since the emergence of legal obligations to protect personal data. Thus, the notion of Privacy-Preserving Data Mining emerged to allow the extraction of knowledge from data without violating the privacy of individuals. Several transformation techniques have been proposed to protect the privacy of individuals. However, their application does not guarantee a null risk of an individual being re-identified. Furthermore, and most importantly, for this paper, the application of such techniques may have a considerable impact on the utility of data and their use in predictive and descriptive tasks. In this paper, we present a study to provide key insights concerning the impact of privacy-preserving techniques in predictive performance. Unlike previous work, our main conclusions point towards a noticeable impact of privacy-preservation techniques in predictive performance.","tags":[],"title":"The Compromise of Data Privacy in Predictive Performance","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://tmcarvalho.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]